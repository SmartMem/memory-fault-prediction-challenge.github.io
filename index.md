---
layout: default 
---

<span style="display:block;text-align:center">![Alt text](assets/logo_smart_mem.jpg "Title")</span>

# Motivation

<p style='text-align: justify;'>
Memory uncorrectable Errors (UEs) have been identified as a major failure cause in
data centers, which highly threaten the availability and reliability of the server and
even the entire computing clusters. Forecasting UEs before enacting preemptive
maintenance measures has emerged as a viable strategy for diminishing server outages
and some machine learning based solution have also been proposed. However,
the UEs prediction presents several challenges: data noise and extremely imbalance
as the UEs are exceedingly rare in memory events; heterogeneous data sources
as the DRAMs in the field come from different manufacturing or architecture
platforms; distribution shifts due to the hardware aging; and latent factors due to
the dynamic access mechanism. We cure a real-world DRAM error dataset that
contains both micro and bit information and present a two-stage challenge for more
efficient and generalized event prediction solution. We believe the competition will
provide a breeding ground to foster discussions and further progress on several
important research topics towards real-world ML applications. </p>

<p style='text-align: justify;'>
This challenge aims to push forward the boundaries of what these powerful LLMs can achieve on edge devices in terms of performance, efficiency, and versatility. Specifically, it targets to deploy LLMs on edge devices in resource-bounded scenarios and address the following major issues:</p>

1. <b>Vast amounts of memory requirements</b>: LLM inference typically requires a significant amount of memory, a major bottleneck for off-the-shelf smartphones. Even high-end smartphones with 8GB are not sufficient for a sophisticated LLM.
2. <b>Prohibitive energy consumption</b>: the significant energy consumption during LLM inference presents a challenge to the battery life of smartphones.
3. <b>Large performance loss</b>: state-of-the-art LLMs might require unusually high compression ratios to fit into the memory of edge devices. Such extremely high compression ratios will significantly challenge the efficacy of existing model compression techniques, where we usually see a substantial performance drop after certain high levels of compression ratios. Therefore, achieving such high compression ratios while maintaining reasonable performance is very challenging.
4. <b>Lack of offline functionality</b>: Most off-the-shelf LLMs necessitate internet connections, which limits their usage in remote areas or in situations where internet access may be intermittent.

# Potential impact

<p style='text-align: justify;'>
This competition aims to generate impacts in science, economics, and society. Scientifically, it may inspire new algorithms and optimizations for LLMs on edge devices, encourage interdisciplinary research, and involve more university experts studying LLMs despite limited resources. Economically, it could reduce energy costs, save on network bandwidth, cloud fees, and data transmission expenses, thus improving affordability and accessibility of LLM applications. Socially, the competition may tackle issues like digital inclusion, privacy, and sustainability by developing solutions promoting accessibility, privacy via on-device processing, and lower carbon footprint due to reduced reliance on centralized cloud infrastructure. By leveraging edge computing and LLMs, competition solutions have the potential to drive positive social change, promote equity, and address the needs of underrepresented communities. </p>

# Scope

<p style='text-align: justify;'>
This competition encourages researchers, practitioners, and industry professionals from a wide range of fields to participate if they are interested in co-designing systems, hardware, and algorithms to enable high-performing LLMs on edge devices, unlocking new possibilities for various industries and use cases. </p>
